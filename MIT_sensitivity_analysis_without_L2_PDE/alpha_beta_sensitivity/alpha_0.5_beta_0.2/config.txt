data: MyMIT
batch_size: 512
normalization_method: min-max
epochs: 200
early_stop: 25
warmup_epochs: 20
warmup_lr: 0.0001
lr: 3e-05
final_lr: 2e-06
lr_F: 1e-05
warmup_epochs_F: 20
warmup_lr_F: 0.0001
final_lr_F: 2e-06
u_layers_num: 3
u_hidden_dim: 60
F_layers_num: 3
F_hidden_dim: 60
alpha: 0.5
beta: 0.2
log_dir: logging.txt
save_folder: MIT_sensitivity_analysis_without_L2_PDE\alpha_beta_sensitivity\alpha_0.5_beta_0.2
